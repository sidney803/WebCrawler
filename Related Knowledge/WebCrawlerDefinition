網路爬蟲（英語：web crawler），也叫網路蜘蛛（spider），是一種用來自動瀏覽全球資訊網的網路機器人。其目的一般為編纂網路索引。

網路搜尋引擎等站點通過爬蟲軟體更新自身的網站內容或其對其他網站的索引。網路爬蟲可以將自己所存取的頁面儲存下來，以便搜尋引擎事後生成索引供使用者搜尋。

爬蟲存取網站的過程會消耗目標系統資源。不少網路系統並不默許爬蟲工作。因此在存取大量頁面時，爬蟲需要考慮到規劃、負載，還需要講「禮貌」。 不願意被爬蟲存取、被爬蟲主人知曉的公開站點可以使用robots.txt檔案之類的方法避免存取。這個檔案可以要求機器人只對網站的一部分進行索引，或完全不作處理。

網際網路上的頁面極多，即使是最大的爬蟲系統也無法做出完整的索引。因此在公元2000年之前的全球資訊網出現初期，搜尋引擎經常找不到多少相關結果。現在的搜尋引擎在這方面已經進步很多，能夠即刻給出高品質結果。

爬蟲還可以驗證超連結和HTML代碼，用於網路抓取（參見資料驅動編程）

網路爬蟲始於一張被稱作種子的統一資源位址（URL）列表。當網路爬蟲存取這些統一資源定位器時，它們會甄別出頁面上所有的超連結，並將它們寫入一張「待訪列表」，即所謂爬行疆域。此疆域上的URL將會被按照一套策略迴圈來存取。如果爬蟲在執行的過程中複製歸檔和儲存網站上的資訊，這些檔案通常儲存，使他們可以較容易的被檢視。閱讀和瀏覽他們儲存的網站上並即時更新的資訊，這些被儲存的網頁又被稱為「快照」。越大容量的網頁意味著網路爬蟲只能在給予的時間內下載越少部分的網頁，所以要優先考慮其下載。高變化率意味著網頁可能已經被更新或者被取代。一些伺服器端軟體生成的URL（統一資源定位符）也使得網路爬蟲很難避免檢索到重複內容。

但是網際網路的資源卷帙浩繁，這也意味著網路爬蟲只能在一定時間內下載有限數量的網頁，因此它需要衡量優先順序的下載方式。有時候網頁出現、更新和消失的速度很快，也就是說網路爬蟲下載的網頁在幾秒後就已經被修改或甚至刪除了。這些都是網路爬蟲設計師們所面臨的兩個問題。

再者，伺服器端軟體所生成的統一資源位址數量龐大，以致網路爬蟲難免也會採集到重複的內容。根據超文字傳輸協定，無盡組合的參數所返回的頁面中，只有很少一部分確實傳回正確的內容。例如：數張快照陳列室的網站，可能通過幾個參數，讓使用者選擇相關快照：其一是通過四種方法對快照排序，其二是關於快照解析度的的三種選擇，其三是兩種檔案格式，另加一個使用者可否提供內容的選擇，這樣對於同樣的結果會有48種（4*3*2）不同的統一資源位址與其關聯。這種數學組合替網路爬蟲造成了麻煩，因為它們必須越過這些無關指令碼變化的組合，尋找不重複的內容。

source: https://zh.wikipedia.org/zh-tw/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2
